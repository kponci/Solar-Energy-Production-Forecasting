{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Group 159 - üê• *(Front-Facing Baby Chick emoji)*\n",
    "## Long Note\n",
    "≈†imon Prasek - 105773\n",
    "Karel Poncar - 105236\n",
    "Miroslav Matƒõjƒçek - 105672"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook you fan find all the methods we used. We tried few using a sklearn and then AutoML H2O. The ones using sklearn are trained on dataset X_train_observed and then crossvalidated on X_train_estimated. H2O is using sklearn function for splitting the dataset to training and testing part.\n",
    "\n",
    "This crossvalidation is tried just on dataset A. Then the solution with the lowest mean absolute error (MAE) is chosen and used for the other datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:02:35.272848357Z",
     "start_time": "2023-11-12T14:02:34.931626958Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T14:02:37.361914033Z",
     "start_time": "2023-11-12T14:02:34.931781517Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h2o\n",
    "import sklearn\n",
    "import matplotlib \n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "# load of our custom functions\n",
    "from solutions.few_regression_types import data_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:02:37.466224774Z",
     "start_time": "2023-11-12T14:02:37.363732353Z"
    }
   },
   "outputs": [],
   "source": [
    "# read dataset A\n",
    "# for simplicity, We use X_train_estimated as test data for cross validation\n",
    "y = pd.read_parquet(\"../../dataset/A/train_targets.parquet\")\n",
    "X_train = pd.read_parquet(\"../../dataset/A/X_train_observed.parquet\")\n",
    "X_test = pd.read_parquet(\"../../dataset/A/X_train_estimated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:02:37.561485819Z",
     "start_time": "2023-11-12T14:02:37.471157810Z"
    }
   },
   "outputs": [],
   "source": [
    "# edit data\n",
    "X_train, y_train = data_preprocess.preprocess_train_data(X_train, y, \"everything\")\n",
    "X_test, y_test = data_preprocess.preprocess_train_data(X_test, y, \"everything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Machine learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:02:38.753161543Z",
     "start_time": "2023-11-12T14:02:37.567390558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "619.5454603900835"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision tree\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred_tree = decision_tree.predict(X_test)\n",
    "\n",
    "mae_tree = np.mean(np.abs(np.array(y_test) - y_pred_tree))\n",
    "mae_tree  # = 616.575890061115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:03:27.021696867Z",
     "start_time": "2023-11-12T14:02:38.752698565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "600.4268668725019"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest\n",
    "random_forest = RandomForestRegressor(n_estimators=100)\n",
    "random_forest.fit(X_train, y_train.values.ravel()) # ravel part is because of scikit's data conversion warning, it does not have to be there\n",
    "y_pred_forest = random_forest.predict(X_test)\n",
    "\n",
    "mae_forest = np.mean(np.abs(np.array(y_test) - y_pred_forest))\n",
    "mae_forest  # = 599.9553060312836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:03:42.596171536Z",
     "start_time": "2023-11-12T14:03:27.022851624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "592.2928322998533"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient boosting\n",
    "gradient_boosting = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1) \n",
    "gradient_boosting.fit(X_train, y_train.values.ravel())\n",
    "y_pred_grad = gradient_boosting.predict(X_test)\n",
    "\n",
    "mae_grad = np.mean(np.abs(np.array(y_test) - y_pred_grad))\n",
    "mae_grad    # = 592.2928322998536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:03:43.475942992Z",
     "start_time": "2023-11-12T14:03:42.595863466Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirek/miniconda3/envs/solar-energy-production/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.684e+09, tolerance: 4.240e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": "599.9464980245718"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elastic net\n",
    "elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)\n",
    "elastic_net.fit(X_train, y_train.values.ravel())\n",
    "y_pred_elast_net = elastic_net.predict(X_test)\n",
    "\n",
    "mae_elast_net = np.mean(np.abs(np.array(y_test) - y_pred_elast_net))\n",
    "mae_elast_net   # = 599.946498024572"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:04:35.419235361Z",
     "start_time": "2023-11-12T14:03:43.477452753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "473.76274242555763"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support vector regression\n",
    "svr_model = SVR(kernel='rbf', C=1.)\n",
    "svr_model.fit(X_train, y_train.values.ravel())\n",
    "y_pred_svr = svr_model.predict(X_test)\n",
    "\n",
    "mae_svr = np.mean(np.abs(np.array(y_test) - y_pred_svr))\n",
    "mae_svr # = 473.76274242555763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:11:27.380468024Z",
     "start_time": "2023-11-12T14:04:35.423938829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345.1870758178765, 362.3174570223702, 397.974746466719, 437.0422504854231, 473.76274242555763, 497.00397971674676, 513.0037055004162, "
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning of SVR\n",
    "# no need to run this section, it takes too long; the results are approx.: \n",
    "# [345, 362, 397, 437, 473, 497, 513] \n",
    "# lower C gives us better results\n",
    "for C in [0.001, 0.03, 0.1, 0.3, 1, 3, 10]:\n",
    "    svr_model = SVR(kernel='rbf', C=C)\n",
    "    svr_model.fit(X_train, y_train.values.ravel())\n",
    "    y_pred_svr = svr_model.predict(X_test)\n",
    "\n",
    "    print(np.mean(np.abs(np.array(y_test) - y_pred_svr)), end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T14:12:30.735533266Z",
     "start_time": "2023-11-12T14:11:27.380192919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/plain": "--------------------------  -----------------------------\nH2O_cluster_uptime:         2 hours 15 mins\nH2O_cluster_timezone:       Europe/Prague\nH2O_data_parsing_timezone:  UTC\nH2O_cluster_version:        3.44.0.2\nH2O_cluster_version_age:    4 days\nH2O_cluster_name:           H2O_from_python_mirek_0utjor\nH2O_cluster_total_nodes:    1\nH2O_cluster_free_memory:    14.96 Gb\nH2O_cluster_total_cores:    12\nH2O_cluster_allowed_cores:  12\nH2O_cluster_status:         locked, healthy\nH2O_connection_url:         http://localhost:54321\nH2O_connection_proxy:       {\"http\": null, \"https\": null}\nH2O_internal_security:      False\nPython_version:             3.10.13 final\n--------------------------  -----------------------------",
      "text/html": "\n<style>\n\n#h2o-table-1.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-1 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-1 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-1 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-1 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-1 .h2o-table th,\n#h2o-table-1 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-1 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-1\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption></caption>\n    <thead></thead>\n    <tbody><tr><td>H2O_cluster_uptime:</td>\n<td>2 hours 15 mins</td></tr>\n<tr><td>H2O_cluster_timezone:</td>\n<td>Europe/Prague</td></tr>\n<tr><td>H2O_data_parsing_timezone:</td>\n<td>UTC</td></tr>\n<tr><td>H2O_cluster_version:</td>\n<td>3.44.0.2</td></tr>\n<tr><td>H2O_cluster_version_age:</td>\n<td>4 days</td></tr>\n<tr><td>H2O_cluster_name:</td>\n<td>H2O_from_python_mirek_0utjor</td></tr>\n<tr><td>H2O_cluster_total_nodes:</td>\n<td>1</td></tr>\n<tr><td>H2O_cluster_free_memory:</td>\n<td>14.96 Gb</td></tr>\n<tr><td>H2O_cluster_total_cores:</td>\n<td>12</td></tr>\n<tr><td>H2O_cluster_allowed_cores:</td>\n<td>12</td></tr>\n<tr><td>H2O_cluster_status:</td>\n<td>locked, healthy</td></tr>\n<tr><td>H2O_connection_url:</td>\n<td>http://localhost:54321</td></tr>\n<tr><td>H2O_connection_proxy:</td>\n<td>{\"http\": null, \"https\": null}</td></tr>\n<tr><td>H2O_internal_security:</td>\n<td>False</td></tr>\n<tr><td>Python_version:</td>\n<td>3.10.13 final</td></tr></tbody>\n  </table>\n</div>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| (done) 100%\n",
      "Parse progress: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| (done) 100%\n",
      "AutoML progress: |\n",
      "15:11:29.145: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| (done) 100%\n",
      "model_id                                                     mae     rmse     mse       rmsle    mean_residual_deviance\n",
      "StackedEnsemble_AllModels_1_AutoML_5_20231112_151129     195.439  436.035  190127  nan                           190127\n",
      "StackedEnsemble_BestOfFamily_2_AutoML_5_20231112_151129  197.03   437.359  191283  nan                           191283\n",
      "GBM_3_AutoML_5_20231112_151129                           197.43   439.29   192976  nan                           192976\n",
      "GBM_2_AutoML_5_20231112_151129                           199.276  441.532  194951  nan                           194951\n",
      "StackedEnsemble_BestOfFamily_1_AutoML_5_20231112_151129  200.111  441.804  195191  nan                           195191\n",
      "GBM_4_AutoML_5_20231112_151129                           201.013  441.96   195329  nan                           195329\n",
      "GBM_1_AutoML_5_20231112_151129                           201.024  443.811  196968  nan                           196968\n",
      "DRF_1_AutoML_5_20231112_151129                           209.709  468.466  219460    0.687323                    219460\n",
      "XGBoost_1_AutoML_5_20231112_151129                       214.674  480.741  231112  nan                           231112\n",
      "XGBoost_2_AutoML_5_20231112_151129                       215.885  479.631  230046  nan                           230046\n",
      "[15 rows x 6 columns]\n",
      "H2O session _sid_ac91 closed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50730/618429429.py:44: H2ODeprecationWarning: Deprecated, use ``h2o.cluster().shutdown()``.\n",
      "  h2o.shutdown()\n"
     ]
    }
   ],
   "source": [
    "#H2O AutoML\n",
    "\n",
    "X = pd.concat([\n",
    "    pd.read_parquet(\"../../dataset/A/X_train_observed.parquet\"),\n",
    "    pd.read_parquet(\"../../dataset/A//X_train_estimated.parquet\")\n",
    "], ignore_index=True)\n",
    "y = pd.read_parquet(\"../../dataset/A//train_targets.parquet\")\n",
    "\n",
    "X_y_bacon = X.merge(y, left_on = \"date_forecast\", right_on = \"time\")\n",
    "X_y_bacon = X_y_bacon.dropna(subset = \"pv_measurement\")\n",
    "\n",
    "columns_to_drop = [\"snow_drift:idx\", \"elevation:m\", 'snow_melt_10min:mm', 'fresh_snow_12h:cm',\n",
    "                   'fresh_snow_3h:cm',  'fresh_snow_6h:cm', 'wind_speed_w_1000hPa:ms', \n",
    "                   'snow_water:kgm2', 'snow_density:kgm3', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm', \n",
    "                   'wind_speed_v_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_10m:ms',\n",
    "                   'msl_pressure:hPa', 'pressure_100m:hPa', 'pressure_50m:hPa', 'dew_or_rime:idx', \n",
    "                   'date_forecast', 'clear_sky_energy_1h:J', 'diffuse_rad_1h:J', 'direct_rad_1h:J', \n",
    "                   'dew_point_2m:K','super_cooled_liquid_water:kgm2', 'ceiling_height_agl:m', 'cloud_base_agl:m', 'prob_rime:p','date_forecast']   \n",
    "\n",
    "X_y_bacon.drop(columns=columns_to_drop+['time'], inplace=True)\n",
    "\n",
    "train,test = sklearn.model_selection.train_test_split(X_y_bacon, test_size = 0.20)\n",
    "\n",
    "h2o.init(max_mem_size = \"16G\") # If u have more RAM change the parameter\n",
    "\n",
    "train_frame = h2o.H2OFrame(train)\n",
    "test_frame = h2o.H2OFrame(test)\n",
    "\n",
    "x = train_frame.columns[:-1] \n",
    "y = train_frame.columns[-1] \n",
    "\n",
    "aml = H2OAutoML(max_runtime_secs = 60,\n",
    "                sort_metric = \"MAE\",\n",
    "                stopping_metric = \"MAE\",\n",
    "                seed = 7213712285)\n",
    "aml.train(x = x, \n",
    "          y = y,\n",
    "          training_frame = train_frame,\n",
    "          validation_frame = test_frame)\n",
    "\n",
    "print(aml.leaderboard)\n",
    "best_model = aml.get_best_model(criterion='MAE')\n",
    "\n",
    "h2o.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prediction on real test data\n",
    "\n",
    "\n",
    "Usage of Support Vector Regression on data sets A,B,C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:16:00.182261578Z",
     "start_time": "2023-11-12T14:12:30.736407192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset A\n",
      "dataset B\n",
      "dataset C\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Regression\n",
    "prediction = []\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # read the data\n",
    "    print(f\"dataset {letter}\")\n",
    "    X_train = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y_train = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    # preprocess the data\n",
    "    X_train, y_train = data_preprocess.preprocess_train_data(X_train, y_train, \"everything\")\n",
    "    X_test = data_preprocess.preprocess_test_data(X_test, \"everything\")\n",
    "    # learn \n",
    "    model = SVR(kernel='rbf', C=.001)\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    prediction = np.concatenate((prediction, model.predict(X_test)))\n",
    "prediction[prediction < 0.] = 0. # energy production can't be negative\n",
    "df = pd.DataFrame({'prediction': prediction})\n",
    "df.to_csv('svr.csv', index_label='id')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Usage of H2O AutoML on data sets A,B,C. There are few versions. We figured out some work better than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# H2O with data preprocess - remove NaNs, columns (V1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"11.0.20.1\" 2023-08-24; OpenJDK Runtime Environment (build 11.0.20.1+1-post-Ubuntu-0ubuntu120.04); OpenJDK 64-Bit Server VM (build 11.0.20.1+1-post-Ubuntu-0ubuntu120.04, mixed mode, sharing)\n",
      "  Starting server from /home/mirek/miniconda3/envs/solar-energy-production/lib/python3.10/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpisrug7s4\n",
      "  JVM stdout: /tmp/tmpisrug7s4/h2o_mirek_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpisrug7s4/h2o_mirek_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/plain": "--------------------------  -----------------------------\nH2O_cluster_uptime:         02 secs\nH2O_cluster_timezone:       Europe/Prague\nH2O_data_parsing_timezone:  UTC\nH2O_cluster_version:        3.44.0.2\nH2O_cluster_version_age:    4 days\nH2O_cluster_name:           H2O_from_python_mirek_6pkjot\nH2O_cluster_total_nodes:    1\nH2O_cluster_free_memory:    16 Gb\nH2O_cluster_total_cores:    12\nH2O_cluster_allowed_cores:  12\nH2O_cluster_status:         locked, healthy\nH2O_connection_url:         http://127.0.0.1:54321\nH2O_connection_proxy:       {\"http\": null, \"https\": null}\nH2O_internal_security:      False\nPython_version:             3.10.13 final\n--------------------------  -----------------------------",
      "text/html": "\n<style>\n\n#h2o-table-2.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-2 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-2 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-2 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-2 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-2 .h2o-table th,\n#h2o-table-2 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-2 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-2\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption></caption>\n    <thead></thead>\n    <tbody><tr><td>H2O_cluster_uptime:</td>\n<td>02 secs</td></tr>\n<tr><td>H2O_cluster_timezone:</td>\n<td>Europe/Prague</td></tr>\n<tr><td>H2O_data_parsing_timezone:</td>\n<td>UTC</td></tr>\n<tr><td>H2O_cluster_version:</td>\n<td>3.44.0.2</td></tr>\n<tr><td>H2O_cluster_version_age:</td>\n<td>4 days</td></tr>\n<tr><td>H2O_cluster_name:</td>\n<td>H2O_from_python_mirek_6pkjot</td></tr>\n<tr><td>H2O_cluster_total_nodes:</td>\n<td>1</td></tr>\n<tr><td>H2O_cluster_free_memory:</td>\n<td>16 Gb</td></tr>\n<tr><td>H2O_cluster_total_cores:</td>\n<td>12</td></tr>\n<tr><td>H2O_cluster_allowed_cores:</td>\n<td>12</td></tr>\n<tr><td>H2O_cluster_status:</td>\n<td>locked, healthy</td></tr>\n<tr><td>H2O_connection_url:</td>\n<td>http://127.0.0.1:54321</td></tr>\n<tr><td>H2O_connection_proxy:</td>\n<td>{\"http\": null, \"https\": null}</td></tr>\n<tr><td>H2O_internal_security:</td>\n<td>False</td></tr>\n<tr><td>Python_version:</td>\n<td>3.10.13 final</td></tr></tbody>\n  </table>\n</div>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset A\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['time'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 38\u001B[0m\n\u001B[1;32m     28\u001B[0m columns_to_drop \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msnow_drift:idx\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124melevation:m\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msnow_melt_10min:mm\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfresh_snow_12h:cm\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     29\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfresh_snow_3h:cm\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfresh_snow_6h:cm\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwind_speed_w_1000hPa:ms\u001B[39m\u001B[38;5;124m'\u001B[39m, \n\u001B[1;32m     30\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msnow_water:kgm2\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msnow_density:kgm3\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfresh_snow_1h:cm\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfresh_snow_24h:cm\u001B[39m\u001B[38;5;124m'\u001B[39m, \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     33\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_forecast\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclear_sky_energy_1h:J\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiffuse_rad_1h:J\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdirect_rad_1h:J\u001B[39m\u001B[38;5;124m'\u001B[39m, \n\u001B[1;32m     34\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdew_point_2m:K\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msuper_cooled_liquid_water:kgm2\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mceiling_height_agl:m\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcloud_base_agl:m\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprob_rime:p\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_forecast\u001B[39m\u001B[38;5;124m'\u001B[39m]   \n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Remove specified columns from training data\u001B[39;00m\n\u001B[0;32m---> 38\u001B[0m \u001B[43mX_y_bacon\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns_to_drop\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtime\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Load test data and remove specified columns\u001B[39;00m\n\u001B[1;32m     41\u001B[0m X_test \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_parquet(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../dataset/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mletter\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/X_test_estimated.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/solar-energy-production/lib/python3.10/site-packages/pandas/core/frame.py:5258\u001B[0m, in \u001B[0;36mDataFrame.drop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   5110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n\u001B[1;32m   5111\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   5112\u001B[0m     labels: IndexLabel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5119\u001B[0m     errors: IgnoreRaise \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   5120\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   5121\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   5122\u001B[0m \u001B[38;5;124;03m    Drop specified labels from rows or columns.\u001B[39;00m\n\u001B[1;32m   5123\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5256\u001B[0m \u001B[38;5;124;03m            weight  1.0     0.8\u001B[39;00m\n\u001B[1;32m   5257\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 5258\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5259\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5260\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5262\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5263\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5264\u001B[0m \u001B[43m        \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5265\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5266\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/solar-energy-production/lib/python3.10/site-packages/pandas/core/generic.py:4549\u001B[0m, in \u001B[0;36mNDFrame.drop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   4547\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m axis, labels \u001B[38;5;129;01min\u001B[39;00m axes\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m   4548\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 4549\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_drop_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4551\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[1;32m   4552\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_inplace(obj)\n",
      "File \u001B[0;32m~/miniconda3/envs/solar-energy-production/lib/python3.10/site-packages/pandas/core/generic.py:4591\u001B[0m, in \u001B[0;36mNDFrame._drop_axis\u001B[0;34m(self, labels, axis, level, errors, only_slice)\u001B[0m\n\u001B[1;32m   4589\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mdrop(labels, level\u001B[38;5;241m=\u001B[39mlevel, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[1;32m   4590\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 4591\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m \u001B[43maxis\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4592\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mget_indexer(new_axis)\n\u001B[1;32m   4594\u001B[0m \u001B[38;5;66;03m# Case for non-unique axis\u001B[39;00m\n\u001B[1;32m   4595\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/solar-energy-production/lib/python3.10/site-packages/pandas/core/indexes/base.py:6699\u001B[0m, in \u001B[0;36mIndex.drop\u001B[0;34m(self, labels, errors)\u001B[0m\n\u001B[1;32m   6697\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39many():\n\u001B[1;32m   6698\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 6699\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(labels[mask])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in axis\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6700\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m indexer[\u001B[38;5;241m~\u001B[39mmask]\n\u001B[1;32m   6701\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelete(indexer)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"['time'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# H2O with data preprocess - remove NaNs, columns (V1)\n",
    "\n",
    "all_predictions = pd.DataFrame()\n",
    "# Init of H2O\n",
    "h2o.init(max_mem_size = \"16G\") # If u have more RAM change the parameter\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # Load data\n",
    "    print(f\"dataset {letter}\")\n",
    "    # Load data from Parquet files and concatenate them into a single DataFrame 'X'\n",
    "    X = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    \n",
    "    # Preprocess the data\n",
    "    \n",
    "    # Merge DataFrame 'X' and 'y' based on the common column \"date_forecast\"\n",
    "    X_y_bacon = X.merge(y, left_on = \"date_forecast\", right_on = \"time\")\n",
    "    \n",
    "    # Remove rows with missing values in the \"pv_measurement\" variable\n",
    "    X_y_bacon = X_y_bacon.dropna(subset = \"pv_measurement\")\n",
    "    X_y_bacon.drop(columns=['time'], inplace=True)\n",
    "    \n",
    "    # List of columns to be dropped\n",
    "   \n",
    "    columns_to_drop = [\"snow_drift:idx\", \"elevation:m\", 'snow_melt_10min:mm','fresh_snow_12h:cm',\n",
    "                       'fresh_snow_3h:cm',  'fresh_snow_6h:cm', 'wind_speed_w_1000hPa:ms', \n",
    "                       'snow_water:kgm2', 'snow_density:kgm3', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm', \n",
    "                       'wind_speed_v_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_10m:ms',\n",
    "                       'msl_pressure:hPa', 'pressure_100m:hPa', 'pressure_50m:hPa', 'dew_or_rime:idx', \n",
    "                       'date_forecast', 'clear_sky_energy_1h:J', 'diffuse_rad_1h:J', 'direct_rad_1h:J', \n",
    "                       'dew_point_2m:K','super_cooled_liquid_water:kgm2', 'ceiling_height_agl:m', 'cloud_base_agl:m', 'prob_rime:p','date_forecast']   \n",
    "    \n",
    "    \n",
    "    # Remove specified columns from training data\n",
    "    X_y_bacon.drop(columns=columns_to_drop+['time'], inplace=True)\n",
    "    \n",
    "    # Load test data and remove specified columns\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    X_test = X_test.iloc[::4]\n",
    "    X_test.drop(columns=columns_to_drop, inplace=True)\n",
    "    x_test = h2o.H2OFrame(X_test)\n",
    "\n",
    "    \n",
    "    # Split data into training and validation frames (80:20)\n",
    "    train,test = sklearn.model_selection.train_test_split(X_y_bacon, test_size = 0.20)\n",
    "\n",
    "    train_frame = h2o.H2OFrame(train)\n",
    "    test_frame = h2o.H2OFrame(test)\n",
    "    \n",
    "    x = train_frame.columns[:-1] \n",
    "    y = train_frame.columns[-1] \n",
    "    \n",
    "    # Create an AutoML model\n",
    "    aml = H2OAutoML(max_runtime_secs = 60,\n",
    "                    sort_metric = \"MAE\",\n",
    "                    stopping_metric = \"MAE\",\n",
    "                    seed = 7213712285) #7213712285 \n",
    "    aml.train(x = x, \n",
    "              y = y,\n",
    "              training_frame = train_frame,\n",
    "              validation_frame = test_frame)\n",
    "    \n",
    "    print(aml.leaderboard)\n",
    "    best_model = aml.get_best_model(criterion='MAE')\n",
    "    second_best_model = h2o.get_model(h2o.as_list(aml.leaderboard)[\"model_id\"].iloc[1])\n",
    "    \n",
    "    # Make predictions on test data and make CSV file\n",
    "    prediction1 = best_model.predict(x_test)\n",
    "    predictions1_df = h2o.as_list(prediction1)\n",
    "    predictions1_df[predictions1_df < 0.] = 0.\n",
    "        \n",
    "    all_predictions = pd.concat([all_predictions, predictions1_df], ignore_index=True)\n",
    "    all_predictions.to_csv('AutoML_H2O-V1.csv', index_label='id')\n",
    "    print(\"CSV file updated\")\n",
    "\n",
    "# Explain the best model on the validation frame for model interpretation\n",
    "best_model.explain(test_frame)\n",
    "\n",
    "# Shut down H2O\n",
    "h2o.shutdown()\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:16:08.318262904Z",
     "start_time": "2023-11-12T14:16:00.189270305Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# H2O with data preprocess and two predictors (V2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T14:16:08.319162531Z",
     "start_time": "2023-11-12T14:16:08.318452383Z"
    }
   },
   "outputs": [],
   "source": [
    "# H2O with data preprocess and two predictors (V2)\n",
    "\n",
    "all_predictions = pd.DataFrame()\n",
    "# Init of H2O\n",
    "h2o.init(max_mem_size = \"16G\") # If u have more RAM change the parameter\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # Load data\n",
    "    print(f\"dataset {letter}\")\n",
    "    # Load data from Parquet files and concatenate them into a single DataFrame 'X'\n",
    "    X = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    \n",
    "    # Preprocess the data\n",
    "    \n",
    "    # Merge DataFrame 'X' and 'y' based on the common column \"date_forecast\"\n",
    "    X_y_bacon = X.merge(y, left_on = \"date_forecast\", right_on = \"time\")\n",
    "    \n",
    "    # Remove rows with missing values in the \"pv_measurement\" variable\n",
    "    X_y_bacon = X_y_bacon.dropna(subset = \"pv_measurement\")\n",
    "    \n",
    "    # List of columns to be dropped\n",
    "   \n",
    "    columns_to_drop = [\"snow_drift:idx\", \"elevation:m\", 'snow_melt_10min:mm','fresh_snow_12h:cm',\n",
    "                       'fresh_snow_3h:cm',  'fresh_snow_6h:cm', 'wind_speed_w_1000hPa:ms', \n",
    "                       'snow_water:kgm2', 'snow_density:kgm3', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm', \n",
    "                       'wind_speed_v_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_10m:ms',\n",
    "                       'msl_pressure:hPa', 'pressure_100m:hPa', 'pressure_50m:hPa', 'dew_or_rime:idx', \n",
    "                       'date_forecast', 'clear_sky_energy_1h:J', 'diffuse_rad_1h:J', 'direct_rad_1h:J', \n",
    "                       'dew_point_2m:K','super_cooled_liquid_water:kgm2', 'ceiling_height_agl:m', 'cloud_base_agl:m', 'prob_rime:p','date_forecast']   \n",
    "    \n",
    "    \n",
    "    # Remove specified columns from training data\n",
    "    X_y_bacon.drop(columns=columns_to_drop+['time'], inplace=True)\n",
    "    \n",
    "    # Load test data and remove specified columns\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    X_test = X_test.iloc[::4]\n",
    "    X_test.drop(columns=columns_to_drop, inplace=True)\n",
    "    x_test = h2o.H2OFrame(X_test)\n",
    "\n",
    "    \n",
    "    # Split data into training and validation frames (80:20)\n",
    "    train,test = sklearn.model_selection.train_test_split(X_y_bacon, test_size = 0.20)\n",
    "\n",
    "    train_frame = h2o.H2OFrame(train)\n",
    "    test_frame = h2o.H2OFrame(test)\n",
    "    \n",
    "    x = train_frame.columns[:-1] \n",
    "    y = train_frame.columns[-1] \n",
    "    \n",
    "    # Create an AutoML model\n",
    "    aml = H2OAutoML(max_runtime_secs = 60,\n",
    "                    sort_metric = \"MAE\",\n",
    "                    stopping_metric = \"MAE\",\n",
    "                    seed = 7213712285) #7213712285\n",
    "    aml.train(x = x, \n",
    "              y = y,\n",
    "              training_frame = train_frame,\n",
    "              validation_frame = test_frame)\n",
    "    \n",
    "    print(aml.leaderboard)\n",
    "    best_model = aml.get_best_model(criterion='MAE')\n",
    "    second_best_model = h2o.get_model(h2o.as_list(aml.leaderboard)[\"model_id\"].iloc[1])\n",
    "    \n",
    "    # Make predictions on test data and make CSV file\n",
    "    prediction1 = best_model.predict(x_test)\n",
    "    predictions1_df = h2o.as_list(prediction1)\n",
    "    predictions1_df[predictions1_df < 0.] = 0.\n",
    "    \n",
    "    prediction2 = second_best_model.predict(x_test)\n",
    "    predictions2_df = h2o.as_list(prediction2)\n",
    "    predictions2_df[predictions2_df < 0.] = 0.\n",
    "\n",
    "    prediction = (predictions1_df[\"predict\"] + predictions2_df[\"predict\"])/2\n",
    "    \n",
    "    all_predictions = pd.concat([all_predictions, prediction], ignore_index=True)\n",
    "    all_predictions.to_csv('AutoML_H2O-V2.csv', index_label='id')\n",
    "    print(\"CSV file updated\")\n",
    "\n",
    "# Explain the best model on the validation frame for model interpretation\n",
    "best_model.explain(test_frame)\n",
    "\n",
    "# Shut down H2O\n",
    "h2o.shutdown()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# H2O without data preprocess - USED IN SHORT NOTE (V3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.320669709Z"
    }
   },
   "outputs": [],
   "source": [
    "# H2O without data preprocess - USED IN SHORT NOTE (V3)\n",
    "\n",
    "all_predictions = pd.DataFrame()\n",
    "# Init of H2O\n",
    "h2o.init(max_mem_size = \"16G\") # If u have more RAM change the parameter\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # Load data\n",
    "    print(f\"dataset {letter}\")\n",
    "    # Load data from Parquet files and concatenate them into a single DataFrame 'X'\n",
    "    X = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    \n",
    "    # Preprocess the data\n",
    "    \n",
    "    # Merge DataFrame 'X' and 'y' based on the common column \"date_forecast\"\n",
    "    X_y_bacon = X.merge(y, left_on = \"date_forecast\", right_on = \"time\")\n",
    "    \n",
    "    # Load test data and remove specified columns\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    X_test = X_test.iloc[::4]\n",
    "    #X_test.drop(columns=columns_to_drop, inplace=True)\n",
    "    x_test = h2o.H2OFrame(X_test)\n",
    "\n",
    "    \n",
    "    # Split data into training and validation frames (80:20)\n",
    "    train,test = sklearn.model_selection.train_test_split(X_y_bacon, test_size = 0.20)\n",
    "\n",
    "    train_frame = h2o.H2OFrame(train)\n",
    "    test_frame = h2o.H2OFrame(test)\n",
    "    \n",
    "    x = train_frame.columns[:-1] \n",
    "    y = train_frame.columns[-1] \n",
    "    \n",
    "    # Create an AutoML model\n",
    "    aml = H2OAutoML(max_runtime_secs = 60,\n",
    "                    sort_metric = \"MAE\",\n",
    "                    stopping_metric = \"MAE\",\n",
    "                    seed = 7213712285) \n",
    "    aml.train(x = x, \n",
    "              y = y,\n",
    "              training_frame = train_frame,\n",
    "              validation_frame = test_frame)\n",
    "    \n",
    "    print(aml.leaderboard)\n",
    "    best_model = aml.get_best_model(criterion='MAE')\n",
    "    \n",
    "    # Make predictions on test data and make CSV file\n",
    "    prediction1 = best_model.predict(x_test)\n",
    "    predictions1_df = h2o.as_list(prediction1)\n",
    "    predictions1_df[predictions1_df < 0.] = 0.\n",
    "    \n",
    "    all_predictions = pd.concat([all_predictions, predictions1_df], ignore_index=True)\n",
    "    all_predictions.to_csv('AutoML_H2O-V3.csv', index_label='id')\n",
    "    print(\"CSV file updated\")\n",
    "\n",
    "# Explain the best model on the validation frame for model interpretation\n",
    "best_model.explain(test_frame)\n",
    "\n",
    "# Shut down H2O\n",
    "h2o.shutdown()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Another possible solutions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:16:08.326044286Z",
     "start_time": "2023-11-12T14:16:08.321942359Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = pd.read_parquet(\"../../dataset/A/train_targets.parquet\")\n",
    "X_train_observed = pd.read_parquet(\"../../dataset/A/X_train_observed.parquet\")\n",
    "X_train_estimated = pd.read_parquet(\"../../dataset/A/X_train_estimated.parquet\")\n",
    "X_test = pd.read_parquet(\"../../dataset/A/X_test_estimated.parquet\")\n",
    "print(f\"Observed {X_train_observed.shape}\")\n",
    "print(f\"Estimated {X_train_estimated.shape}\")\n",
    "print(f\"y {y_train.shape}\")\n",
    "print(f\"Test {X_train_observed.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.323520915Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we took a deeper look on the data we found out, that snow_drift:idx are only 0. elevation:m is constant, so we can remove them, but we should add a way to distinguish the 3 buildings afterward (probably is_a, is_b and is_c columns) if we use all 3 to train 1 model.\n",
    " \n",
    "Then we did correlation of the parameters and regression. We figured out we are not using wind and snow parameters a lot.\n",
    "But the snow should be a good indicator of low energy production, we should try to connect its features into one: is_snow \n",
    "\n",
    "All the pressure features are really high correlated, so we can use only one of them. I chose the surface pressure\n",
    "\n",
    "All clear_sky_energy_1h:J and clear_sky_rad:W are also really high correlated, lest drop the 1h value, the same for direct_rad and diffuse_rad\n",
    "\n",
    "Humidity and dew_point are also highly correlated, and We don't think we need them both, let's drop the dew_point_2m:K \n",
    "\n",
    "And We are not sure if super_cooled_liquid_water:kgm2 ceiling_height_agl:m cloud_base_agl:m prob_rime:p can have some impact to energy production, but let's assume they do not \n",
    "\n",
    "\n",
    "dew_or_rime:idx is column with 1 for dew and -1 for rime, mby we should split this to two indexes\n",
    "\n",
    "we should split datetime to year, day and hour\n",
    "\n",
    "and we can add vector of ones, that can help for some regression techniques\n",
    "\n",
    "So we made a function:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    \n",
    "    X['snow_density:kgm3'] = X['snow_density:kgm3'].fillna(0)\n",
    "    X = X.interpolate(axis=0)\n",
    "\n",
    "    def is_snow(row):\n",
    "        if row['fresh_snow_24h:cm'] > 0 or row['snow_depth:cm'] > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    X['is_snow'] = X.apply(is_snow, axis=1)\n",
    "    \n",
    "    def is_dew(row):\n",
    "        return 1 if row['dew_or_rime:idx'] > 0.1 else 0\n",
    "    \n",
    "    def is_rime(row):\n",
    "        return 1 if row['dew_or_rime:idx'] < -0.1 else 0\n",
    "    X['is_dew'] = X.apply(is_dew, axis=1)\n",
    "    X['is_rime'] = X.apply(is_rime, axis=1)\n",
    "    \n",
    "    X['year'] = X['date_forecast'].apply(lambda x: x.year)\n",
    "    X['day_of_year'] = X['date_forecast'].apply(lambda x: x.dayofyear)\n",
    "    X['hour_of_day'] = X['date_forecast'].apply(lambda x: x.hour)\n",
    "    \n",
    "    columns_to_drop = [\"snow_drift:idx\", \"elevation:m\", 'snow_melt_10min:mm', 'fresh_snow_12h:cm', 'fresh_snow_3h:cm',  'fresh_snow_6h:cm', 'wind_speed_w_1000hPa:ms', 'snow_water:kgm2', 'snow_density:kgm3', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',  'wind_speed_v_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_10m:ms', 'msl_pressure:hPa', 'pressure_100m:hPa', 'pressure_50m:hPa', 'dew_or_rime:idx', 'date_forecast', 'clear_sky_energy_1h:J', 'diffuse_rad_1h:J', 'direct_rad_1h:J', 'dew_point_2m:K']\n",
    "    columns_to_drop += ['super_cooled_liquid_water:kgm2', 'ceiling_height_agl:m', 'cloud_base_agl:m', 'prob_rime:p']\n",
    "    \n",
    "    X.drop(columns=columns_to_drop, inplace=True)\n",
    "    X.drop([\"date_calc\"], axis=1, inplace=True, errors='ignore')\n",
    "    \n",
    "    X['ones'] = 1\n",
    "        \n",
    "    return X\n",
    "\n",
    "def normalise(X):\n",
    "    return StandardScaler().fit_transform(X)\n",
    "\n",
    "def make_y_15mins(y):\n",
    "    y = y.loc[y.index.repeat(4)].reset_index(drop=True)\n",
    "    y['time'] += y.groupby(y.index // 4).cumcount() * pd.Timedelta(\"15 min\")\n",
    "    \n",
    "    y = y.dropna()\n",
    "    return y\n",
    "\n",
    "\n",
    "def match_x_and_y(X, y):\n",
    "    y = y.dropna()\n",
    "    X = X[X['date_forecast'].isin(y['time'])].reset_index(drop=True)\n",
    "    y = y[y['time'].isin(X['date_forecast'])].reset_index(drop=True)\n",
    "    return X, y\n",
    "\n",
    "def drop_time(y):\n",
    "    return y.drop('time', axis=1)\n",
    "\n",
    "def compute_mne(pred, truth):\n",
    "    pred[pred < 0] = 0\n",
    "    mae_nn = np.mean(np.abs(np.array(truth) - pred))\n",
    "    return mae_nn\n",
    "\n",
    "def mean_15min_output(y):\n",
    "    y[y < 0.] = 0\n",
    "    meany = np.zeros(y.shape[0] // 4)\n",
    "    \n",
    "    for i in range(meany.shape[0]):\n",
    "        meany[i] = np.mean(y[4*i: 4*(i+1)])\n",
    "    \n",
    "    return meany\n",
    "\n",
    "def mean_15minX(X):\n",
    "    ret = X.iloc[:,1:].groupby(np.arange(len(X))//4).mean()\n",
    "    ret.insert(0, 'date_forecast',list(X.loc[::4, 'date_forecast']))\n",
    "    return ret\n",
    "    \n",
    "def create_csv(y, name='model.csv'):\n",
    "    y[y < 0.] = 0.\n",
    "    output = pd.DataFrame({'prediction': y})\n",
    "    output.to_csv(name, index_label='id')\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.324786512Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we try it on linear regression:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = []\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # read the data\n",
    "    print(f\"dataset {letter}\")\n",
    "    X_train = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y_train = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    y_train = make_y_15mins(y_train)\n",
    "    X_train, y_train = match_x_and_y(X_train, y_train)\n",
    "    y_train = drop_time(y_train)\n",
    "    X_train = preprocess(X_train)\n",
    "    X_test = preprocess(X_test)\n",
    "    \n",
    "    ML = LinearRegression()\n",
    "    model = ML.fit(X_train, y_train)   \n",
    "    if letter == 'A':\n",
    "        prediction = model.predict(X_test)\n",
    "    else:\n",
    "        prediction = np.vstack((prediction, model.predict(X_test)))\n",
    "\n",
    "prediction = mean_15min_output(prediction)\n",
    "print(prediction.shape)\n",
    "prediction[prediction < 0.] = 0. # energy production can't be negative\n",
    "df = pd.DataFrame({'prediction': prediction.flatten()})\n",
    "df.to_csv('dropped_columns_regression.csv', index_label='id')\n",
    "print(\"done\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.371872622Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also on Random Forest Regressor:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = []\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # read the data\n",
    "    print(f\"dataset {letter}\")\n",
    "    X_train = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y_train = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    y_train = make_y_15mins(y_train)\n",
    "    X_train, y_train = match_x_and_y(X_train, y_train)\n",
    "    X_train = preprocess(X_train)\n",
    "    X_test = preprocess(X_test)\n",
    "    \n",
    "    \n",
    "    ML = RandomForestRegressor(random_state = 42)\n",
    "    model = ML.fit(X_train, y_train['pv_measurement'])   \n",
    "    if letter == 'A':\n",
    "        prediction = model.predict(X_test)\n",
    "    else:\n",
    "        prediction = np.concatenate((prediction, model.predict(X_test)))\n",
    "    \n",
    "\n",
    "prediction = mean_15min_output(prediction)\n",
    "print(prediction.shape)\n",
    "prediction[prediction < 0.] = 0. # energy production can't be negative\n",
    "df = pd.DataFrame({'prediction': prediction.flatten()})\n",
    "df.to_csv('dropped_columns_forest.csv', index_label='id')\n",
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.372038426Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could also try to do one big hour features row instead of meaning four smaller"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = pd.read_parquet(\"../../dataset/A/train_targets.parquet\")\n",
    "X_train_observed = pd.read_parquet(\"../../dataset/A/X_train_observed.parquet\")\n",
    "\n",
    "def join_X_hour(X):\n",
    "    extra_columns = ['hour_of_day', 'day_of_year', 'year']\n",
    "\n",
    "    X1 = X_train_observed[::4].reset_index()\n",
    "    X2 = X_train_observed[1::4].reset_index().add_suffix(\"_+15\").drop(columns=extra_columns)\n",
    "    X3 = X_train_observed[2::4].reset_index().add_suffix(\"_+30\").drop(columns=extra_columns)\n",
    "    X4 = X_train_observed[3::4].reset_index().add_suffix(\"_+45\").drop(columns=extra_columns)\n",
    "\n",
    "    return  pd.concat([X1,X2,X3,X4], axis=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.372131873Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "y_train = pd.read_parquet(\"../../dataset/A/train_targets.parquet\")\n",
    "X_train_observed = pd.read_parquet(\"../../dataset/A/X_train_observed.parquet\")\n",
    "y_test = pd.read_parquet(\"../../dataset/A/train_targets.parquet\")\n",
    "x_test = pd.read_parquet(\"../../dataset/A/X_train_estimated.parquet\")\n",
    "\n",
    "\n",
    "y_train = make_y_15mins(y_train)\n",
    "X_train_observed, y_train = match_x_and_y(X_train_observed, y_train)\n",
    "X_train_observed = preprocess(X_train_observed)\n",
    "cols = X_train_observed.columns\n",
    "\n",
    "y_test = make_y_15mins(y_test)\n",
    "x_test, y_test = match_x_and_y(x_test, y_test)\n",
    "x_test = preprocess(x_test)\n",
    "\n",
    "\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [8,  100, 500],\n",
    "    'max_features': [2,  3, 20],\n",
    "    'min_samples_leaf': [2 , 4, 20],\n",
    "    'min_samples_split': [2, 8 , 20],\n",
    "    'n_estimators': [10, 100, 1000]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = RandomizedSearchCV(estimator = rf, param_distributions = param_grid, \n",
    "                         cv = 3, n_jobs = -1, verbose = 2, return_train_score=True)\n",
    "grid_search.fit(X_train_observed, y_train['pv_measurement']);\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.372206394Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "\"\"\"best_params = {'n_estimators': 1000,\n",
    " 'min_samples_split': 8,\n",
    " 'min_samples_leaf': 2,\n",
    " 'max_features': 3,\n",
    " 'max_depth': 100,\n",
    " 'bootstrap': True}\"\"\"\n",
    "best_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T14:16:08.416102079Z",
     "start_time": "2023-11-12T14:16:08.372322261Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_search.cv_results_).sort_values('rank_test_score')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.372453423Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = []\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # read the data\n",
    "    print(f\"dataset {letter}\")\n",
    "    X_train = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y_train = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    y_train = make_y_15mins(y_train)\n",
    "    X_train, y_train = match_x_and_y(X_train, y_train)\n",
    "    X_train = preprocess(X_train)\n",
    "    X_test = preprocess(X_test)\n",
    "    \n",
    "    \n",
    "    ML = RandomForestRegressor(random_state = 42, **best_params )\n",
    "    model = ML.fit(X_train, y_train['pv_measurement'])   \n",
    "    if letter == 'A':\n",
    "        prediction = model.predict(X_test)\n",
    "    else:\n",
    "        prediction = np.concatenate((prediction, model.predict(X_test)))\n",
    "    \n",
    "\n",
    "prediction = mean_15min_output(prediction)\n",
    "print(prediction.shape)\n",
    "prediction[prediction < 0.] = 0. # energy production can't be negative\n",
    "df = pd.DataFrame({'prediction': prediction.flatten()})\n",
    "df.to_csv('random_search_params_forest.csv', index_label='id')\n",
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.372589544Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do mean of every 4 rows instead of meaning results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = pd.read_parquet(f\"../../dataset/A/X_train_observed.parquet\")\n",
    "mean_15minX(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.372721195Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = []\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # read the data\n",
    "    print(f\"dataset {letter}\")\n",
    "    X_train = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y_train = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    print(X_test.shape)\n",
    "\n",
    "    X_train = mean_15minX(X_train)\n",
    "    X_train, y_train = match_x_and_y(X_train, y_train)\n",
    "    X_train = preprocess(X_train)\n",
    "    X_test = X_test.drop('date_calc', axis=1)\n",
    "    X_test = mean_15minX(X_test)\n",
    "    X_test = preprocess(X_test)\n",
    "\n",
    "    ML = RandomForestRegressor(random_state=42, **best_params)\n",
    "    model = ML.fit(X_train, y_train['pv_measurement'])\n",
    "    print(mean_absolute_error(list(model.predict(X_train)), list(y_train['pv_measurement'])))\n",
    "    if letter == 'A':\n",
    "        prediction = model.predict(X_test)\n",
    "    else:\n",
    "        prediction = np.concatenate((prediction, model.predict(X_test)))\n",
    "\n",
    "#prediction = mean_15min_output(prediction)\n",
    "print(prediction.shape)\n",
    "prediction[prediction < 0.] = 0.  # energy production can't be negative\n",
    "df = pd.DataFrame({'prediction': prediction.flatten()})\n",
    "df.to_csv('meaned_X_dropped_column_random_forrest.csv', index_label='id')\n",
    "print(\"done\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.372850961Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try esemble learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = []\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "\n",
    "\n",
    "linreg_pipeline=make_pipeline(\n",
    "    StandardScaler(), LinearRegression()\n",
    ")\n",
    "lasso_pipeline=make_pipeline(\n",
    "    StandardScaler(), LassoCV()\n",
    ")\n",
    "\n",
    "estimators = [('Random Forest', RandomForestRegressor(random_state=42)),\n",
    "              ('Lasso', lasso_pipeline),\n",
    "              ('Gradient Boosting', HistGradientBoostingRegressor(random_state=42)),\n",
    "              ('Linear Regression', LinearRegression()),\n",
    "              ('Adaboost', AdaBoostRegressor(random_state=42)),\n",
    "              ]\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # read the data\n",
    "    print(f\"dataset {letter}\")\n",
    "    X_train = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y_train = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    print(X_test.shape)\n",
    "\n",
    "    X_train = mean_15minX(X_train)\n",
    "    X_train, y_train = match_x_and_y(X_train, y_train)\n",
    "    X_train = preprocess(X_train)\n",
    "    X_test = X_test.drop('date_calc', axis=1)\n",
    "    X_test = mean_15minX(X_test)\n",
    "    X_test = preprocess(X_test)\n",
    "    \n",
    "    \n",
    "\n",
    "    stacking_regressor = StackingRegressor(estimators=estimators,\n",
    "                                       final_estimator=RidgeCV())\n",
    "\n",
    "    \n",
    "    model = stacking_regressor.fit(X_train, y_train['pv_measurement'])\n",
    "    print(mean_absolute_error(list(model.predict(X_train)), list(y_train['pv_measurement'])))\n",
    "    if letter == 'A':\n",
    "        prediction = model.predict(X_test)\n",
    "    else:\n",
    "        prediction = np.concatenate((prediction, model.predict(X_test)))\n",
    "\n",
    "#prediction = mean_15min_output(prediction)\n",
    "print(prediction.shape)\n",
    "prediction[prediction < 0.] = 0.  # energy production can't be negative\n",
    "df = pd.DataFrame({'prediction': prediction.flatten()})\n",
    "df.to_csv('stacking_droped_meanedX.csv', index_label='id')\n",
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.373013063Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#The same with meaning y instead\n",
    "prediction = []\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "\n",
    "\n",
    "linreg_pipeline=make_pipeline(\n",
    "    StandardScaler(), LinearRegression()\n",
    ")\n",
    "lasso_pipeline=make_pipeline(\n",
    "    StandardScaler(), LassoCV()\n",
    ")\n",
    "\n",
    "estimators = [('Random Forest', RandomForestRegressor(random_state=42)),\n",
    "              ('Lasso', lasso_pipeline),\n",
    "              ('Gradient Boosting', HistGradientBoostingRegressor(random_state=42)),\n",
    "              ('Linear Regression', LinearRegression()),\n",
    "              ('Adaboost', AdaBoostRegressor(random_state=42)),\n",
    "              ]\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # read the data\n",
    "    print(f\"dataset {letter}\")\n",
    "    X_train = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y_train = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    print(X_test.shape)\n",
    "\n",
    "    y_train = make_y_15mins(y_train)\n",
    "    X_train, y_train = match_x_and_y(X_train, y_train)\n",
    "    X_train = preprocess(X_train)\n",
    "    X_test = X_test.drop('date_calc', axis=1)\n",
    "    X_test = preprocess(X_test)\n",
    "    \n",
    "    \n",
    "\n",
    "    stacking_regressor = StackingRegressor(estimators=estimators,\n",
    "                                       final_estimator=RidgeCV())\n",
    "\n",
    "    \n",
    "    model = stacking_regressor.fit(X_train, y_train['pv_measurement'])\n",
    "    print(mean_absolute_error(list(model.predict(X_train)), list(y_train['pv_measurement'])))\n",
    "    if letter == 'A':\n",
    "        prediction = model.predict(X_test)\n",
    "    else:\n",
    "        prediction = np.concatenate((prediction, model.predict(X_test)))\n",
    "\n",
    "prediction = mean_15min_output(prediction)\n",
    "print(prediction.shape)\n",
    "prediction[prediction < 0.] = 0.  # energy production can't be negative\n",
    "df = pd.DataFrame({'prediction': prediction.flatten()})\n",
    "df.to_csv('stacking_droped_meanedY.csv', index_label='id')\n",
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T14:16:08.373128301Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
