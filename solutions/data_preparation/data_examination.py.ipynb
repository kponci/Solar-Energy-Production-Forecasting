{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T15:38:14.756616857Z",
     "start_time": "2023-10-11T15:38:13.814487194Z"
    }
   },
   "id": "8ab3bccfcfbc3cce"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed (118669, 46)\n",
      "Estimated (17576, 47)\n",
      "y (34085, 2)\n",
      "Test (118669, 46)\n"
     ]
    }
   ],
   "source": [
    "y_train = pd.read_parquet(\"../../dataset/A/train_targets.parquet\")\n",
    "X_train_observed = pd.read_parquet(\"../../dataset/A/X_train_observed.parquet\")\n",
    "X_train_estimated = pd.read_parquet(\"../../dataset/A/X_train_estimated.parquet\")\n",
    "X_test = pd.read_parquet(\"../../dataset/A/X_test_estimated.parquet\")\n",
    "print(f\"Observed {X_train_observed.shape}\")\n",
    "print(f\"Estimated {X_train_estimated.shape}\")\n",
    "print(f\"y {y_train.shape}\")\n",
    "print(f\"Test {X_train_observed.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T15:39:35.217283020Z",
     "start_time": "2023-10-11T15:39:35.173697214Z"
    }
   },
   "id": "8dd9835e3c86d20a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is an extra column in estimated "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d30b09b798e07398"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set(X_train_estimated.columns) ^ set(X_train_observed.columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3eefe86ef324c739"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_observed.head(100)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb794fa22d494c58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_estimated.head(10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7470d943c41fae8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We shouldn't need date calc at all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fca4051b348f37f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_estimated.drop([\"date_calc\"], axis=1, inplace=True)\n",
    "X_train_estimated.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40f0074066e807bb"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                  time  pv_measurement\n0  2019-06-02 22:00:00            0.00\n1  2019-06-02 23:00:00            0.00\n2  2019-06-03 00:00:00            0.00\n3  2019-06-03 01:00:00            0.00\n4  2019-06-03 02:00:00           19.36\n5  2019-06-03 03:00:00          251.02\n6  2019-06-03 04:00:00          263.78\n7  2019-06-03 05:00:00          522.72\n8  2019-06-03 06:00:00          904.42\n9  2019-06-03 07:00:00         1238.82\n10 2019-06-03 08:00:00         2189.88\n11 2019-06-03 09:00:00         3047.22\n12 2019-06-03 10:00:00         2163.26\n13 2019-06-03 11:00:00         2686.64\n14 2019-06-03 12:00:00         3175.92\n15 2019-06-03 13:00:00         2730.86\n16 2019-06-03 14:00:00         2093.96\n17 2019-06-03 15:00:00         2774.20\n18 2019-06-03 16:00:00         1833.48\n19 2019-06-03 17:00:00         1057.54\n20 2019-06-03 18:00:00          340.12\n21 2019-06-03 19:00:00           48.40\n22 2019-06-03 20:00:00           12.54\n23 2019-06-03 21:00:00            0.00\n24 2019-06-03 22:00:00            0.00\n25 2019-06-03 23:00:00            0.00\n26 2019-06-04 00:00:00            0.00\n27 2019-06-04 01:00:00            6.38\n28 2019-06-04 02:00:00          117.04\n29 2019-06-04 03:00:00          305.58\n30 2019-06-04 04:00:00          805.42\n31 2019-06-04 05:00:00         1556.94\n32 2019-06-04 06:00:00         2812.04\n33 2019-06-04 07:00:00         2807.64\n34 2019-06-04 08:00:00         4561.26\n35 2019-06-04 09:00:00         4812.06\n36 2019-06-04 10:00:00         4766.52\n37 2019-06-04 11:00:00         4339.72\n38 2019-06-04 12:00:00         2464.00\n39 2019-06-04 13:00:00         2361.04\n40 2019-06-04 14:00:00         1921.92\n41 2019-06-04 15:00:00         1918.18\n42 2019-06-04 16:00:00          681.34\n43 2019-06-04 17:00:00          226.38\n44 2019-06-04 18:00:00          235.84\n45 2019-06-04 19:00:00          220.22\n46 2019-06-04 20:00:00           38.94\n47 2019-06-04 21:00:00            0.00\n48 2019-06-04 22:00:00            0.00\n49 2019-06-04 23:00:00            0.00\n50 2019-06-05 00:00:00            0.00\n51 2019-06-05 01:00:00           10.78\n52 2019-06-05 02:00:00          104.28\n53 2019-06-05 03:00:00          284.24\n54 2019-06-05 04:00:00          807.84\n55 2019-06-05 05:00:00         1336.28\n56 2019-06-05 06:00:00         1399.42\n57 2019-06-05 07:00:00         1189.76\n58 2019-06-05 08:00:00         1719.74\n59 2019-06-05 09:00:00         1232.44\n60 2019-06-05 10:00:00         1139.60\n61 2019-06-05 11:00:00          972.18\n62 2019-06-05 12:00:00         1572.56\n63 2019-06-05 13:00:00         3044.80\n64 2019-06-05 14:00:00         2132.02\n65 2019-06-05 15:00:00         1995.40\n66 2019-06-05 16:00:00          343.42\n67 2019-06-05 17:00:00            0.00\n68 2019-06-05 18:00:00            0.00\n69 2019-06-05 19:00:00            0.00\n70 2019-06-05 20:00:00            0.00\n71 2019-06-05 21:00:00            0.00\n72 2019-06-05 22:00:00            0.00\n73 2019-06-05 23:00:00            0.00\n74 2019-06-06 00:00:00            0.00\n75 2019-06-06 01:00:00            0.00\n76 2019-06-06 02:00:00            0.00\n77 2019-06-06 03:00:00            0.00\n78 2019-06-06 04:00:00            0.00\n79 2019-06-06 05:00:00            0.00\n80 2019-06-06 06:00:00            0.00\n81 2019-06-06 07:00:00            0.00\n82 2019-06-06 08:00:00            0.00\n83 2019-06-06 09:00:00            0.00\n84 2019-06-06 10:00:00            0.00\n85 2019-06-06 11:00:00            0.00\n86 2019-06-06 12:00:00            0.00\n87 2019-06-06 13:00:00            0.00\n88 2019-06-06 14:00:00          102.52\n89 2019-06-06 15:00:00          209.44\n90 2019-06-06 16:00:00          989.78\n91 2019-06-06 17:00:00          317.02\n92 2019-06-06 18:00:00          370.26\n93 2019-06-06 19:00:00          130.68\n94 2019-06-06 20:00:00           67.76\n95 2019-06-06 21:00:00            0.00\n96 2019-06-06 22:00:00            0.00\n97 2019-06-06 23:00:00            0.00\n98 2019-06-07 00:00:00            0.00\n99 2019-06-07 01:00:00            2.42",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>pv_measurement</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-06-02 22:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-06-02 23:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-06-03 00:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-06-03 01:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-06-03 02:00:00</td>\n      <td>19.36</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2019-06-03 03:00:00</td>\n      <td>251.02</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2019-06-03 04:00:00</td>\n      <td>263.78</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2019-06-03 05:00:00</td>\n      <td>522.72</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2019-06-03 06:00:00</td>\n      <td>904.42</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2019-06-03 07:00:00</td>\n      <td>1238.82</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2019-06-03 08:00:00</td>\n      <td>2189.88</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2019-06-03 09:00:00</td>\n      <td>3047.22</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2019-06-03 10:00:00</td>\n      <td>2163.26</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2019-06-03 11:00:00</td>\n      <td>2686.64</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2019-06-03 12:00:00</td>\n      <td>3175.92</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2019-06-03 13:00:00</td>\n      <td>2730.86</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2019-06-03 14:00:00</td>\n      <td>2093.96</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2019-06-03 15:00:00</td>\n      <td>2774.20</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2019-06-03 16:00:00</td>\n      <td>1833.48</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2019-06-03 17:00:00</td>\n      <td>1057.54</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2019-06-03 18:00:00</td>\n      <td>340.12</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2019-06-03 19:00:00</td>\n      <td>48.40</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2019-06-03 20:00:00</td>\n      <td>12.54</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2019-06-03 21:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2019-06-03 22:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2019-06-03 23:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2019-06-04 00:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2019-06-04 01:00:00</td>\n      <td>6.38</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>2019-06-04 02:00:00</td>\n      <td>117.04</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>2019-06-04 03:00:00</td>\n      <td>305.58</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>2019-06-04 04:00:00</td>\n      <td>805.42</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>2019-06-04 05:00:00</td>\n      <td>1556.94</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>2019-06-04 06:00:00</td>\n      <td>2812.04</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>2019-06-04 07:00:00</td>\n      <td>2807.64</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>2019-06-04 08:00:00</td>\n      <td>4561.26</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>2019-06-04 09:00:00</td>\n      <td>4812.06</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>2019-06-04 10:00:00</td>\n      <td>4766.52</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>2019-06-04 11:00:00</td>\n      <td>4339.72</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>2019-06-04 12:00:00</td>\n      <td>2464.00</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>2019-06-04 13:00:00</td>\n      <td>2361.04</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>2019-06-04 14:00:00</td>\n      <td>1921.92</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>2019-06-04 15:00:00</td>\n      <td>1918.18</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>2019-06-04 16:00:00</td>\n      <td>681.34</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>2019-06-04 17:00:00</td>\n      <td>226.38</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>2019-06-04 18:00:00</td>\n      <td>235.84</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>2019-06-04 19:00:00</td>\n      <td>220.22</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>2019-06-04 20:00:00</td>\n      <td>38.94</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>2019-06-04 21:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>2019-06-04 22:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>2019-06-04 23:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>2019-06-05 00:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>2019-06-05 01:00:00</td>\n      <td>10.78</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>2019-06-05 02:00:00</td>\n      <td>104.28</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>2019-06-05 03:00:00</td>\n      <td>284.24</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>2019-06-05 04:00:00</td>\n      <td>807.84</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>2019-06-05 05:00:00</td>\n      <td>1336.28</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>2019-06-05 06:00:00</td>\n      <td>1399.42</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>2019-06-05 07:00:00</td>\n      <td>1189.76</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>2019-06-05 08:00:00</td>\n      <td>1719.74</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>2019-06-05 09:00:00</td>\n      <td>1232.44</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>2019-06-05 10:00:00</td>\n      <td>1139.60</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>2019-06-05 11:00:00</td>\n      <td>972.18</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>2019-06-05 12:00:00</td>\n      <td>1572.56</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>2019-06-05 13:00:00</td>\n      <td>3044.80</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>2019-06-05 14:00:00</td>\n      <td>2132.02</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>2019-06-05 15:00:00</td>\n      <td>1995.40</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>2019-06-05 16:00:00</td>\n      <td>343.42</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>2019-06-05 17:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>2019-06-05 18:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>2019-06-05 19:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>2019-06-05 20:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>2019-06-05 21:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>2019-06-05 22:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>2019-06-05 23:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>2019-06-06 00:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>2019-06-06 01:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>2019-06-06 02:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>2019-06-06 03:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>2019-06-06 04:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>2019-06-06 05:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>2019-06-06 06:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>2019-06-06 07:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>2019-06-06 08:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>2019-06-06 09:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>2019-06-06 10:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>2019-06-06 11:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>2019-06-06 12:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>2019-06-06 13:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>2019-06-06 14:00:00</td>\n      <td>102.52</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>2019-06-06 15:00:00</td>\n      <td>209.44</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>2019-06-06 16:00:00</td>\n      <td>989.78</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>2019-06-06 17:00:00</td>\n      <td>317.02</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>2019-06-06 18:00:00</td>\n      <td>370.26</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>2019-06-06 19:00:00</td>\n      <td>130.68</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>2019-06-06 20:00:00</td>\n      <td>67.76</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>2019-06-06 21:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>2019-06-06 22:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>2019-06-06 23:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>2019-06-07 00:00:00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>2019-06-07 01:00:00</td>\n      <td>2.42</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T15:39:37.976792420Z",
     "start_time": "2023-10-11T15:39:37.957506514Z"
    }
   },
   "id": "3274e5a739aeff3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(y_train.shape[0] * 4)\n",
    "print(X_train_observed.shape[0] + X_train_estimated.shape[0])\n",
    "print(y_train.iloc[0,0], y_train.iloc[-1,0])\n",
    "print(X_train_observed.iloc[0,0], X_train_observed.iloc[-1,0])\n",
    "print(X_train_estimated.iloc[0,0], X_train_estimated.iloc[-1,0])\n",
    "print(X_test.iloc[0,0], X_test.iloc[-1,0])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e589cee23dda18f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dimensions of X and y does not match.\n",
    "We have the weather for every 15 minutes, but doing hourly predictions -> we need to deal with that\n",
    "For now just repeat the lines of y and then we can average the output\n",
    "Also there are some missing times in X or Y -> lets delete them, but do not forget, that thy are missing if we need to use the time data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa05acb8f4820ad4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# y_train = y_train.loc[y_train.index.repeat(4)]\n",
    "# print(y_train.shape)\n",
    "# for i in range(int(y_train.shape[0] / 4)):\n",
    "#     y_train.iloc[4*i+1, 0] += pd.Timedelta(\"15 min\")\n",
    "#     y_train.iloc[4*i+2, 0] += pd.Timedelta(\"30 min\")\n",
    "#     y_train.iloc[4*i+3, 0] += pd.Timedelta(\"45 min\")\n",
    "\n",
    "y_train = y_train.loc[y_train.index.repeat(4)].reset_index(drop=True)\n",
    "y_train['time'] += y_train.groupby(y_train.index // 4).cumcount() * pd.Timedelta(\"15 min\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d71c0c83215c9605"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = y_train[y_train['time'].isin(X_train_observed['date_forecast']) | y_train['time'].isin(X_train_estimated['date_forecast'])].reset_index(drop=True)      \n",
    "X_train_observed = X_train_observed[X_train_observed['date_forecast'].isin(y_train['time'])].reset_index(drop=True)\n",
    "X_train_estimated = X_train_estimated[X_train_estimated['date_forecast'].isin(y_train['time'])].reset_index(drop=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "473cc8824c1d00f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print(y_train.shape[0])\n",
    "print(X_train_observed.shape[0] + X_train_estimated.shape[0])\n",
    "print(y_train.iloc[0,0], y_train.iloc[-1,0])\n",
    "print(X_train_observed.iloc[0,0], X_train_observed.iloc[-1,0])\n",
    "print(X_train_estimated.iloc[0,0], X_train_estimated.iloc[-1,0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c6351e8f15596b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_train_observed[~ X_train_observed['date_forecast'].isin(y_train['time'])])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dab7d90f1452303d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set(X_train_observed.columns[X_train_observed.isna().any()]) & set(X_train_estimated.columns[X_train_estimated.isna().any()].tolist())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1903b29237544222"
  },
  {
   "cell_type": "markdown",
   "source": [
    "These tree columns contains Nans, need make them numbers or remove them"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7f1f8a5e829bd8e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "de9808d0e14d51a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can put snow density to 0, if there is now snow"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f847e39bb60e57fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_observed['snow_density:kgm3'] = X_train_observed['snow_density:kgm3'].fillna(0)\n",
    "X_train_estimated['snow_density:kgm3'] = X_train_estimated['snow_density:kgm3'].fillna(0)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a35d3d49aad3f099"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_observed[X_train_observed['ceiling_height_agl:m'].isna()].head(10)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49693a570cbe76e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_observed[X_train_observed['cloud_base_agl:m'].isna()].shape == X_train_observed[X_train_observed['ceiling_height_agl:m'].isna()].shape\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e7a19a2082a8555"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting a single feature\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 10), sharex=True)\n",
    "feature_name = 'cloud_base_agl:m'\n",
    "X_train_observed[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[0], title=feature_name, color='red')\n",
    "X_train_estimated[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[0], title=feature_name, color='blue')\n",
    "X_test[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[0], title=feature_name, color='green')\n",
    "\n",
    "# Plotting a single feature\n",
    "feature_name = 'ceiling_height_agl:m'\n",
    "X_train_observed[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[1], title=feature_name, color='red')\n",
    "X_train_estimated[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[1], title=feature_name, color='blue')\n",
    "X_test[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[1], title=feature_name, color='green')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee780cde2bb7d9b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Don't know, how to interpret this, we can use interpolate function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a700d06a69c3ead"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_observed = X_train_observed.interpolate(axis=0)\n",
    "X_train_estimated = X_train_estimated.interpolate(axis=0)\n",
    "# Plotting a single feature\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 10), sharex=True)\n",
    "feature_name = 'cloud_base_agl:m'\n",
    "X_train_observed[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[0], title=feature_name, color='red')\n",
    "X_train_estimated[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[0], title=feature_name, color='blue')\n",
    "X_test[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[0], title=feature_name, color='green')\n",
    "\n",
    "# Plotting a single feature\n",
    "feature_name = 'ceiling_height_agl:m'\n",
    "X_train_observed[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[1], title=feature_name, color='red')\n",
    "X_train_estimated[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[1], title=feature_name, color='blue')\n",
    "X_test[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[1], title=feature_name, color='green')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b19306a2873f9bb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look to some information about the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42f4c75fdd9ca36f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_observed.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "345486c7dfeabb4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see, that snow_drift:idx are only 0. elevation:m is constant, we can remove that too, but we should add a way to distinguish the 3 buildings afterwards (brobably is_a, is_b and is_c columns) if we use all 3 to train 1 model "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78ad0d9308222753"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_estimated.drop(columns=[\"snow_drift:idx\", \"elevation:m\"],  inplace=True)\n",
    "X_train_observed.drop(columns=[\"snow_drift:idx\", \"elevation:m\"], inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dde846023eb90add"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "827f869915ae55bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some of the features like (for example wind) should not be necessary and some could be duplicated - lets look at it\n",
    "Let's print its correlation with the solution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aab2c02543da73ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(pd.merge(y_train, X_train_observed.rename(columns={'date_forecast': 'time'}), how=\"inner\", on='time').corr())\n",
    "with open('correlation.html', 'w') as f: # make it nicer\n",
    "    print(pd.merge(y_train, X_train_observed.rename(columns={'date_forecast': 'time'}), how=\"inner\", on='time').corr().style.background_gradient()\n",
    ".to_html(), file=f)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b31958acb9b9ab6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also lets see, which features would linear regression model use, ass its easy to interpret"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "784605942b53b8f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_mne(pred, truth):\n",
    "    pred[pred < 0] = 0\n",
    "    mae_nn = np.mean(np.abs(np.array(truth) - pred))\n",
    "    return mae_nn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48b11ca74c86d7ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_observed\n",
    "reg = LinearRegression().fit(X_train_observed.drop(\"date_forecast\", axis=1), y_train[y_train['time'].isin(X_train_observed['date_forecast'])]['pv_measurement'].reset_index(drop=True))\n",
    "reg.score(X_train_observed.drop(\"date_forecast\", axis=1), y_train[y_train['time'].isin(X_train_observed['date_forecast'])]['pv_measurement'].reset_index(drop=True))\n",
    "\n",
    "res = reg.predict(X_train_observed.drop(\"date_forecast\", axis=1))\n",
    "compute_mne(res, y_train[y_train['time'].isin(X_train_observed['date_forecast'])]['pv_measurement'].reset_index(drop=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61d831283d85f21f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try this regression use on test data for fun"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36d2428361b35ede"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test.drop(columns=['date_calc', 'snow_drift:idx', 'elevation:m'], inplace=True, errors='ignore')\n",
    "X_test['snow_density:kgm3'] = X_test['snow_density:kgm3'].fillna(0)\n",
    "X_test = X_test.interpolate(axis=0)\n",
    "pred4 = reg.predict(X_test.drop(\"date_forecast\", axis=1))\n",
    "\n",
    "prediction = np.zeros(pred4.shape[0] // 4)\n",
    "\n",
    "for i in range(prediction.shape[0]):\n",
    "    prediction[i] = np.mean(pred4[4*i: 4*(i+1)])\n",
    "\n",
    "prediction[prediction < 0.] = 0.\n",
    "prediction = np.hstack([prediction, prediction, prediction])\n",
    "df = pd.DataFrame({'prediction': prediction})\n",
    "df.to_csv('easiest_regression.csv', index_label='id')\n",
    "prediction.shape\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e881a12c3b11431c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a6ca4c5ea5946f23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "coefs = reg.coef_\n",
    "#coefs *= np.mean(X_train_observed.drop(\"date_forecast\", axis=1), axis=0) \n",
    "for feature, coef in sorted(zip(reg.feature_names_in_, reg.coef_), key=lambda x:abs(x[1]), reverse=True):\n",
    "    print(feature, coef)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a102b432fdbbb4ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With normalisation\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c20df88239fac5cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_observed\n",
    "regNorm = LinearRegression().fit(StandardScaler().fit_transform(X_train_observed.drop(\"date_forecast\", axis=1)), y_train[y_train['time'].isin(X_train_observed['date_forecast'])]['pv_measurement'].reset_index(drop=True))\n",
    "regNorm.score(StandardScaler().fit_transform(X_train_observed.drop(\"date_forecast\", axis=1)), y_train[y_train['time'].isin(X_train_observed['date_forecast'])]['pv_measurement'].reset_index(drop=True))\n",
    "\n",
    "resNorm = regNorm.predict(StandardScaler().fit_transform(X_train_observed.drop(\"date_forecast\", axis=1)))\n",
    "compute_mne(resNorm, y_train[y_train['time'].isin(X_train_observed['date_forecast'])]['pv_measurement'].reset_index(drop=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94674f53aa4b299b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#X_test.drop(columns=['date_calc', 'snow_drift:idx', 'elevation:m'], inplace=True)\n",
    "X_test['snow_density:kgm3'] = X_test['snow_density:kgm3'].fillna(0)\n",
    "X_test = X_test.interpolate(axis=0)\n",
    "pred4Norm = regNorm.predict(StandardScaler().fit_transform(X_test.drop(\"date_forecast\", axis=1)))\n",
    "\n",
    "predictionNorm = np.zeros(pred4.shape[0] // 4)\n",
    "\n",
    "for i in range(predictionNorm.shape[0]):\n",
    "    predictionNorm[i] = np.mean(pred4[4*i: 4*(i+1)])\n",
    "\n",
    "predictionNorm[predictionNorm < 0.] = 0.\n",
    "predictionNormN = np.hstack([predictionNorm, predictionNorm, predictionNorm])\n",
    "df = pd.DataFrame({'prediction': predictionNorm})\n",
    "df.to_csv('easiest_regression.csv', index_label='id')\n",
    "prediction.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8748bba5b53c2c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for feature, coef in sorted(zip(reg.feature_names_in_, regNorm.coef_), key=lambda x:abs(x[1]), reverse=True):\n",
    "    print(feature, coef)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "858596aa97b2664d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "According to the correlation and regression parameters, we are not using wind and snow parameters a lot.\n",
    "But the snow should be a good indicator of low energy production, we should try to connect it's features into one: is_snow \n",
    "\n",
    "All of the preassure features are really high correlated, so we can use only one of them. I choosed the surface preasure\n",
    "\n",
    "All clear_sky_energy_1h:J and clear_sky_rad:W are also really high correlated, lest drop the 1h value, the same for direct_rad and diffuse_rad\n",
    "\n",
    "Humidity and dew_point are also highly correlated and I don't think we need them both, let's drop the dew_point_2m:K \n",
    "\n",
    "And I am not sure if super_cooled_liquid_water:kgm2 ceiling_height_agl:m cloud_base_agl:m prob_rime:p can have some impact to energy production, but let's assume they do not \n",
    "\n",
    "\n",
    "dew_or_rime:idx is column with 1 for dew and -1 for rime, mby we should split this to two indexes\n",
    "\n",
    "we should split datetime to year, day and hour\n",
    "\n",
    "and we can add vector of ones, that can help for some regression techniques\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a811915987afdff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's make a function, that make's it all also with previous changes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fcab2dd6c9b866b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    \n",
    "    X['snow_density:kgm3'] = X['snow_density:kgm3'].fillna(0)\n",
    "    X = X.interpolate(axis=0)\n",
    "\n",
    "    def is_snow(row):\n",
    "        if row['fresh_snow_24h:cm'] > 0 or row['snow_depth:cm'] > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    X['is_snow'] = X.apply(is_snow, axis=1)\n",
    "    \n",
    "    def is_dew(row):\n",
    "        return 1 if row['dew_or_rime:idx'] > 0.1 else 0\n",
    "    \n",
    "    def is_rime(row):\n",
    "        return 1 if row['dew_or_rime:idx'] < -0.1 else 0\n",
    "    X['is_dew'] = X.apply(is_dew, axis=1)\n",
    "    X['is_rime'] = X.apply(is_rime, axis=1)\n",
    "    \n",
    "    X['year'] = X['date_forecast'].apply(lambda x: x.year)\n",
    "    X['day_of_year'] = X['date_forecast'].apply(lambda x: x.dayofyear)\n",
    "    X['hour_of_day'] = X['date_forecast'].apply(lambda x: x.hour)\n",
    "    \n",
    "    columns_to_drop = [\"snow_drift:idx\", \"elevation:m\", 'snow_melt_10min:mm', 'fresh_snow_12h:cm', 'fresh_snow_3h:cm',  'fresh_snow_6h:cm', 'wind_speed_w_1000hPa:ms', 'snow_water:kgm2', 'snow_density:kgm3', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',  'wind_speed_v_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_10m:ms', 'msl_pressure:hPa', 'pressure_100m:hPa', 'pressure_50m:hPa', 'dew_or_rime:idx', 'date_forecast', 'clear_sky_energy_1h:J', 'diffuse_rad_1h:J', 'direct_rad_1h:J', 'dew_point_2m:K']\n",
    "    columns_to_drop += ['super_cooled_liquid_water:kgm2', 'ceiling_height_agl:m', 'cloud_base_agl:m', 'prob_rime:p']\n",
    "    \n",
    "    X.drop(columns=columns_to_drop, inplace=True)\n",
    "    X.drop([\"date_calc\"], axis=1, inplace=True, errors='ignore')\n",
    "    \n",
    "    X['ones'] = 1\n",
    "        \n",
    "    return X\n",
    "\n",
    "def normalise(X):\n",
    "    return StandardScaler().fit_transform(X)\n",
    "\n",
    "def make_y_15mins(y):\n",
    "    y = y.loc[y.index.repeat(4)].reset_index(drop=True)\n",
    "    y['time'] += y.groupby(y.index // 4).cumcount() * pd.Timedelta(\"15 min\")\n",
    "    \n",
    "    y = y.dropna()\n",
    "    return y\n",
    "\n",
    "\n",
    "def match_x_and_y(X, y):\n",
    "    X = X[X['date_forecast'].isin(y['time'])].reset_index(drop=True)\n",
    "    y = y[y['time'].isin(X['date_forecast'])].reset_index(drop=True)\n",
    "    return X, y\n",
    "\n",
    "def drop_time(y):\n",
    "    return y.drop('time', axis=1)\n",
    "\n",
    "def compute_mne(pred, truth):\n",
    "    pred[pred < 0] = 0\n",
    "    mae_nn = np.mean(np.abs(np.array(truth) - pred))\n",
    "    return mae_nn\n",
    "\n",
    "def mean_15min_output(y):\n",
    "    y[y < 0.] = 0\n",
    "    meany = np.zeros(y.shape[0] // 4)\n",
    "    \n",
    "    for i in range(meany.shape[0]):\n",
    "        meany[i] = np.mean(y[4*i: 4*(i+1)])\n",
    "    \n",
    "    return meany\n",
    "    \n",
    "def create_csv(y, name='model.csv'):\n",
    "    y[y < 0.] = 0.\n",
    "    output = pd.DataFrame({'prediction': y})\n",
    "    output.to_csv(name, index_label='id')\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T15:38:20.714799296Z",
     "start_time": "2023-10-11T15:38:20.695212162Z"
    }
   },
   "id": "2b6ff1c2d845ce09"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And let's make everything again and see, the coeficients now"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7113533ba6e7a83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "y_train = pd.read_parquet(\"../../dataset/A/train_targets.parquet\")\n",
    "X_train_observed = pd.read_parquet(\"../../dataset/A/X_train_observed.parquet\")\n",
    "\n",
    "y_train = make_y_15mins(y_train)\n",
    "X_train_observed, y_train = match_x_and_y(X_train_observed, y_train)\n",
    "X_train_observed = preprocess(X_train_observed)\n",
    "y_train = drop_time(y_train)\n",
    "cols = X_train_observed.columns\n",
    "#X_train_observed = normalise(X_train_observed)\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_observed, y_train)\n",
    "print(reg.score(X_train_observed, y_train))\n",
    "\n",
    "res = reg.predict(X_train_observed)\n",
    "print(f'MNE: {compute_mne(resNorm, y_train)} \\n')\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.hstack([X_train_observed.iloc[25:50, 1:2], resNorm[25:50,0:1], y_train.iloc[25:50,0:1]]))\n",
    "\n",
    "coefs = reg.coef_[0]\n",
    "\n",
    "#coefs*= np.mean(X_train_observed, axis=0) # lets multiply the coefs by values on the coll, so we can see how much impact it has\n",
    "\n",
    "for feature, coef in sorted(zip(cols, coefs), key=lambda x:abs(x[1]), reverse=True):\n",
    "    print(feature, coef)\n",
    "    \n",
    "    \n",
    "#print(X_train_observed)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b09db8f4621556"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok cannot interpret this, but it works somehow, let's try it "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78bdf50263ec6be7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset A\n",
      "(2880, 47)\n",
      "dataset B\n",
      "(2880, 47)\n",
      "dataset C\n",
      "(2880, 47)\n",
      "(2160,)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # read the data\n",
    "    print(f\"dataset {letter}\")\n",
    "    X_train = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y_train = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    y_train = make_y_15mins(y_train)\n",
    "    X_train, y_train = match_x_and_y(X_train, y_train)\n",
    "    y_train = drop_time(y_train)\n",
    "    X_train = preprocess(X_train)\n",
    "    X_test = preprocess(X_test)\n",
    "    \n",
    "    ML = LinearRegression()\n",
    "    model = ML.fit(X_train, y_train)   \n",
    "    if letter == 'A':\n",
    "        prediction = model.predict(X_test)\n",
    "    else:\n",
    "        prediction = np.vstack((prediction, model.predict(X_test)))\n",
    "\n",
    "prediction = mean_15min_output(prediction)\n",
    "print(prediction.shape)\n",
    "prediction[prediction < 0.] = 0. # energy production can't be negative\n",
    "df = pd.DataFrame({'prediction': prediction.flatten()})\n",
    "df.to_csv('dropped_columns_regression.csv', index_label='id')\n",
    "print(\"done\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T15:38:40.863514441Z",
     "start_time": "2023-10-11T15:38:30.964146732Z"
    }
   },
   "id": "9c85d09db85a4da2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try the same on different ML than regression, and try to find the best parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2fb0af4263b6c2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "y_train = pd.read_parquet(\"../../dataset/A/train_targets.parquet\")\n",
    "X_train_observed = pd.read_parquet(\"../../dataset/A/X_train_observed.parquet\")\n",
    "y_test = pd.read_parquet(\"../../dataset/A/train_targets.parquet\")\n",
    "x_test = pd.read_parquet(\"../../dataset/A/X_train_estimated.parquet\")\n",
    "\n",
    "\n",
    "y_train = make_y_15mins(y_train)\n",
    "X_train_observed, y_train = match_x_and_y(X_train_observed, y_train)\n",
    "X_train_observed = preprocess(X_train_observed)\n",
    "cols = X_train_observed.columns\n",
    "\n",
    "y_test = make_y_15mins(y_test)\n",
    "x_test, y_test = match_x_and_y(x_test, y_test)\n",
    "x_test = preprocess(x_test)\n",
    "\n",
    "\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "#grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "#                          cv = 3, n_jobs = -1, verbose = 2, return_train_score=True)\n",
    "#grid_search.fit(X_train_observed, y_train['pv_measurement']);\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edc14ccec8cbf7"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset A\n",
      "(2880, 47)\n",
      "dataset B\n",
      "(2880, 47)\n",
      "dataset C\n",
      "(2880, 47)\n",
      "(2160,)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # read the data\n",
    "    print(f\"dataset {letter}\")\n",
    "    X_train = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y_train = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    y_train = make_y_15mins(y_train)\n",
    "    X_train, y_train = match_x_and_y(X_train, y_train)\n",
    "    X_train = preprocess(X_train)\n",
    "    X_test = preprocess(X_test)\n",
    "    \n",
    "    ML = RandomForestRegressor(random_state = 42)\n",
    "    model = ML.fit(X_train, y_train['pv_measurement'])   \n",
    "    if letter == 'A':\n",
    "        prediction = model.predict(X_test)\n",
    "    else:\n",
    "        prediction = np.concatenate((prediction, model.predict(X_test)))\n",
    "    \n",
    "\n",
    "prediction = mean_15min_output(prediction)\n",
    "print(prediction.shape)\n",
    "prediction[prediction < 0.] = 0. # energy production can't be negative\n",
    "df = pd.DataFrame({'prediction': prediction.flatten()})\n",
    "df.to_csv('dropped_columns_forest.csv', index_label='id')\n",
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T15:57:59.699212927Z",
     "start_time": "2023-10-11T15:51:43.710065897Z"
    }
   },
   "id": "d735230a0e0a646f"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "57f3021af5ffcef5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could also try to do one big hour features row instead of meaning four smaller"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4ff73c9f3b9ea93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = pd.read_parquet(\"../../dataset/A/train_targets.parquet\")\n",
    "X_train_observed = pd.read_parquet(\"../../dataset/A/X_train_observed.parquet\")\n",
    "\n",
    "def join_X_hour(X):\n",
    "    extra_columns = ['hour_of_day', 'day_of_year', 'year']\n",
    "\n",
    "    X1 = X_train_observed[::4].reset_index()\n",
    "    X2 = X_train_observed[1::4].reset_index().add_suffix(\"_+15\").drop(columns=extra_columns)\n",
    "    X3 = X_train_observed[2::4].reset_index().add_suffix(\"_+30\").drop(columns=extra_columns)\n",
    "    X4 = X_train_observed[3::4].reset_index().add_suffix(\"_+45\").drop(columns=extra_columns)\n",
    "\n",
    "    return  pd.concat([X1,X2,X3,X4], axis=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fc0a6097230b098"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9a926018011fa991"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
