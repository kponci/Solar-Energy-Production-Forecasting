{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this notebook, few regression types are tried using sklearn. They are trained on dataset X_train_observed and then crossvalidated on X_train_estimated (there is no particular reason behind dividing the dataset to training and testing part like this, I just decided it would be easiest way). \n",
    "\n",
    "This crossvalidation is tried just on dataset A. Then the solution with the lowest mean absolute error (mae) is chosen and used for the other datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:28:14.052302930Z",
     "start_time": "2023-10-10T05:28:14.008635872Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T13:39:39.820930500Z",
     "start_time": "2023-11-06T13:39:39.810327Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "# load my custom functions\n",
    "from solutions.few_regression_types import data_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T20:38:47.047352500Z",
     "start_time": "2023-11-04T20:38:46.784200400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# read dataset A\n",
    "# for simplicity, I use X_train_estimated as test data for cross validation\n",
    "y = pd.read_parquet(\"../../dataset/A/train_targets.parquet\")\n",
    "X_train = pd.read_parquet(\"../../dataset/A/X_train_observed.parquet\")\n",
    "X_test = pd.read_parquet(\"../../dataset/A/X_train_estimated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:07:36.838666874Z",
     "start_time": "2023-10-10T05:07:36.284106209Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# edit data\n",
    "X_train, y_train = data_preprocess.preprocess_train_data(X_train, y, \"everything\")\n",
    "X_test, y_test = data_preprocess.preprocess_train_data(X_test, y, \"everything\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:07:36.860814491Z",
     "start_time": "2023-10-10T05:07:36.617097333Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (29667, 47)\n",
      "X_test.shape = (4394, 47)\n",
      "y_train.shape = (29667, 1)\n",
      "y_test.shape = (4394, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train.shape = {X_train.shape}\")\n",
    "print(f\"X_test.shape = {X_test.shape}\")\n",
    "print(f\"y_train.shape = {y_train.shape}\")\n",
    "print(f\"y_test.shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Machine learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:07:38.029789376Z",
     "start_time": "2023-10-10T05:07:36.627646761Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622.6471723254433"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decision tree\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred_tree = decision_tree.predict(X_test)\n",
    "\n",
    "mae_tree = np.mean(np.abs(np.array(y_test) - y_pred_tree))\n",
    "mae_tree    # mae means mean absolute error, mae_tree = 616.575890061115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:08:43.351614721Z",
     "start_time": "2023-10-10T05:07:38.022561362Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600.4668224222045"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random forest\n",
    "random_forest = RandomForestRegressor(n_estimators=100)\n",
    "random_forest.fit(X_train, y_train.values.ravel()) # ravel part is because of scikit's data conversion warning, it does not have to be there\n",
    "y_pred_forest = random_forest.predict(X_test)\n",
    "\n",
    "mae_forest = np.mean(np.abs(np.array(y_test) - y_pred_forest))\n",
    "mae_forest  # 599.9553060312836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:09:00.460070793Z",
     "start_time": "2023-10-10T05:08:43.291886470Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592.2928322998534"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient boosting\n",
    "gradient_boosting = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1) \n",
    "gradient_boosting.fit(X_train, y_train.values.ravel())\n",
    "y_pred_grad = gradient_boosting.predict(X_test)\n",
    "\n",
    "mae_grad = np.mean(np.abs(np.array(y_test) - y_pred_grad))\n",
    "mae_grad    # 592.2928322998536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:09:04.735282781Z",
     "start_time": "2023-10-10T05:09:00.421149131Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\OneDrive\\Dokumenty\\Solar-Energy-Production-Forecasting\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.684e+09, tolerance: 4.240e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "599.9464980245721"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elastic net\n",
    "elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)\n",
    "elastic_net.fit(X_train, y_train.values.ravel())\n",
    "y_pred_elast_net = elastic_net.predict(X_test)\n",
    "\n",
    "mae_elast_net = np.mean(np.abs(np.array(y_test) - y_pred_elast_net))\n",
    "mae_elast_net   # 599.946498024572"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:10:55.779718796Z",
     "start_time": "2023-10-10T05:09:04.675976472Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473.76274242555763"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# support vector regression\n",
    "svr_model = SVR(kernel='rbf', C=1.)\n",
    "svr_model.fit(X_train, y_train.values.ravel())\n",
    "y_pred_svr = svr_model.predict(X_test)\n",
    "\n",
    "mae_svr = np.mean(np.abs(np.array(y_test) - y_pred_svr))\n",
    "mae_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:21:20.763820351Z",
     "start_time": "2023-10-10T05:10:55.789288673Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345.1870758178765, 362.3174570223702, 397.974746466719, 437.0422504854231, 473.76274242555763, 497.00397971674676, 513.0037055004162, "
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning of SVR\n",
    "# no need to run this section, it takes too long; the results are approx.: \n",
    "# [345, 362, 397, 437, 473, 497, 513] \n",
    "# lower C gives us better results\n",
    "for C in [0.001, 0.03, 0.1, 0.3, 1, 3, 10]:\n",
    "    svr_model = SVR(kernel='rbf', C=C)\n",
    "    svr_model.fit(X_train, y_train.values.ravel())\n",
    "    y_pred_svr = svr_model.predict(X_test)\n",
    "\n",
    "    print(np.mean(np.abs(np.array(y_test) - y_pred_svr)), end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-5.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-5 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-5 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-5 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table th,\n",
       "#h2o-table-5 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>7 mins 25 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Prague</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.44.0.1</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>21 days, 4 hours and 22 minutes</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_simon_ac7n4x</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>13.88 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.11.5 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -------------------------------\n",
       "H2O_cluster_uptime:         7 mins 25 secs\n",
       "H2O_cluster_timezone:       Europe/Prague\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.44.0.1\n",
       "H2O_cluster_version_age:    21 days, 4 hours and 22 minutes\n",
       "H2O_cluster_name:           H2O_from_python_simon_ac7n4x\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    13.88 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.11.5 final\n",
       "--------------------------  -------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |█\n",
      "14:49:07.466: AutoML: XGBoost is not available; skipping it.\n",
      "14:49:07.506: _train param, Dropping bad and constant columns: [snow_drift:idx, elevation:m]\n",
      "14:49:08.415: _train param, Dropping bad and constant columns: [snow_drift:idx, elevation:m]\n",
      "14:49:10.626: _train param, Dropping bad and constant columns: [snow_drift:idx, elevation:m]\n",
      "\n",
      "██████████\n",
      "14:49:21.957: _train param, Dropping bad and constant columns: [snow_drift:idx, elevation:m]\n",
      "14:49:22.861: _train param, Dropping bad and constant columns: [snow_drift:idx, elevation:m]\n",
      "14:49:24.68: _train param, Dropping unused columns: [snow_drift:idx, elevation:m]\n",
      "14:49:24.681: _train param, Dropping unused columns: [snow_drift:idx, elevation:m]\n",
      "\n",
      "████████████████████████████████████████████████████| (done) 100%\n",
      "model_id                                                    rmse     mse      mae     rmsle    mean_residual_deviance\n",
      "GBM_1_AutoML_1_20231106_144907                           319.885  102326  124.306  nan                         102326\n",
      "StackedEnsemble_AllModels_1_AutoML_1_20231106_144907     322.244  103841  123.526  nan                         103841\n",
      "GBM_2_AutoML_1_20231106_144907                           322.776  104184  124.737  nan                         104184\n",
      "GBM_3_AutoML_1_20231106_144907                           324.494  105296  124.298  nan                         105296\n",
      "StackedEnsemble_BestOfFamily_1_AutoML_1_20231106_144907  326.308  106477  126.145  nan                         106477\n",
      "DRF_1_AutoML_1_20231106_144907                           340.694  116072  130.22     0.6862                    116072\n",
      "GLM_1_AutoML_1_20231106_144907                           420.206  176573  233.177  nan                         176573\n",
      "[7 rows x 6 columns]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_7100\\72466135.py:24: H2ODeprecationWarning: Deprecated, use ``h2o.cluster().shutdown()``.\n",
      "  h2o.shutdown()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_b502 closed.\n"
     ]
    }
   ],
   "source": [
    "#H2O AutoML\n",
    "\n",
    "y = pd.read_parquet(\"../../dataset/A/train_targets.parquet\")\n",
    "X_train = pd.read_parquet(\"../../dataset/A/X_train_observed.parquet\")\n",
    "X_test = pd.read_parquet(\"../../dataset/A/X_train_estimated.parquet\")\n",
    "\n",
    "X_train, y_train = data_preprocess.preprocess_train_data(X_train, y, \"everything\")\n",
    "X_test, y_test = data_preprocess.preprocess_train_data(X_test, y, \"everything\")\n",
    "\n",
    "h2o.init(max_mem_size = \"16G\") # If u have more RAM change the parameter\n",
    "\n",
    "train_frame = h2o.H2OFrame(pd.concat([X_train, y_train], axis=1))\n",
    "test_frame = h2o.H2OFrame(pd.concat([X_test, y_test], axis=1))\n",
    "\n",
    "x = train_frame.columns[:-1] \n",
    "y = train_frame.columns[-1] \n",
    "\n",
    "aml = H2OAutoML(max_runtime_secs=60,max_models = 5)\n",
    "aml.train(x=x, y=y, training_frame=train_frame, validation_frame=test_frame)\n",
    "\n",
    "print(aml.leaderboard)\n",
    "best_model = aml.leader\n",
    "\n",
    "h2o.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Prediction on real test data\n",
    "\n",
    "\n",
    "SVR model came out with the lowest mean absolute error. So far we did only cross validation on the training data. SVR will be used on the real test data, on the datasets B and C and to generate the output csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T05:46:01.145833814Z",
     "start_time": "2023-10-10T05:41:15.610533094Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prediction = []\n",
    "\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # read the data\n",
    "    print(f\"dataset {letter}\")\n",
    "    X_train = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y_train = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    # preprocess the data\n",
    "    X_train, y_train = data_preprocess.preprocess_train_data(X_train, y_train, \"everything\")\n",
    "    X_test = data_preprocess.preprocess_test_data(X_test, \"everything\")\n",
    "    # learn \n",
    "    model = SVR(kernel='rbf', C=.001)\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    prediction = np.concatenate((prediction, model.predict(X_test)))\n",
    "prediction[prediction < 0.] = 0. # energy production can't be negative\n",
    "df = pd.DataFrame({'prediction': prediction})\n",
    "df.to_csv('svr.csv', index_label='id')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1029579131.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 15\u001b[1;36m\u001b[0m\n\u001b[1;33m    h2o.init(nfolds: \"-1\", max_mem_size = \"16G\") # If u have more RAM change the parameter,\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    # read the data\n",
    "    print(f\"dataset {letter}\")\n",
    "    X_train = pd.concat([\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_observed.parquet\"),\n",
    "        pd.read_parquet(f\"../../dataset/{letter}/X_train_estimated.parquet\")\n",
    "    ], ignore_index=True)\n",
    "    y_train = pd.read_parquet(f\"../../dataset/{letter}/train_targets.parquet\")\n",
    "    X_test = pd.read_parquet(f\"../../dataset/{letter}/X_test_estimated.parquet\")\n",
    "    # preprocess the data\n",
    "    X_train, y_train = data_preprocess.preprocess_train_data(X_train, y_train, \"everything\")\n",
    "    X_test = data_preprocess.preprocess_test_data(X_test, \"everything\")\n",
    "    \n",
    "    h2o.init(max_mem_size = \"16G\") # If u have more RAM change the parameter\n",
    "\n",
    "    train_frame = h2o.H2OFrame(pd.concat([X_train, y_train], axis=1))\n",
    "    test_frame = h2o.H2OFrame(pd.concat([X_test, y_test], axis=1))\n",
    "\n",
    "    x = train_frame.columns[:-1] \n",
    "    y = train_frame.columns[-1] \n",
    "\n",
    "    model1 = H2OAutoML(max_runtime_secs=60)\n",
    "    model1.train(x=x, y=y, training_frame=train_frame, validation_frame=test_frame)\n",
    "\n",
    "    print(model1.leaderboard)\n",
    "    prediction = best_model.predict(test_frame)#,seed = 123456) # idk why, but the seed doesnt work for me\n",
    "\n",
    "    predictions_df = h2o.as_list(prediction)\n",
    "    \n",
    "    predictions_df['prediction'][predictions_df['prediction'] < 0] = 0\n",
    "    \n",
    "    predictions_df.to_csv('automl.csv', index_label='id')\n",
    "\n",
    "    print(\"done\")\n",
    "    h2o.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
